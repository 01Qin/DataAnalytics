{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìå 1. Loading the Dataset",
   "id": "8ebca1c6d8223b49"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from os.path import join as path_join\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bokeh.core.property.primitive import String\n",
    "from holoviews.ipython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, kstest, pearsonr, spearmanr\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error,accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) # suppress mathematically undefined errors\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset from URL\n",
    "data_root = kagglehub.dataset_download(\"andrewkronser/cve-common-vulnerabilities-and-exposures\")\n"
   ],
   "id": "5ce79fb3c069149c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üí° **Interpretation**:",
   "id": "a57b8e557f12efe1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **mod_date: The date the entry was last modified.**\n",
    "- **pub_date: The date the entry was published.**\n",
    "- **cvss: Common Vulnerability Scoring System (CVSS) score, which measures the severity of a vulnerability.**\n",
    "- **cwe_code: Common Weakness Enumeration (CWE) code identifying the type of weakness.**\n",
    "- **cwe_name: The name associated with the CWE code.**\n",
    "- **summary: A text summary of the vulnerability.**\n",
    "- **access_authentication: Indicates whether authentication is required.**\n",
    "- **access_complexity: How difficult the attack is to execute.**\n",
    "- **access_vector: How the attack is performed(e.g., via network or locally).**"
   ],
   "id": "2bc3f463846a9b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the dataset into a pandas dataframe\n",
    "df = pd.read_csv(path_join(data_root, 'cve.csv'), header=0, index_col=0)\n",
    "df.mod_date = pd.to_datetime(df.mod_date)\n",
    "df.pub_date = pd.to_datetime(df.pub_date)\n",
    "\n",
    "# Check for missing values before summary statistics\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# General information about the DataFrame (column names, non-null counts, data types)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "\n",
    "non_par_com = {\"NONE\": 0, \"COMPLETE\": 2, \"PARTIAL\": 1}\n",
    "low_med_hih = {\"LOW\": 0, \"MEDIUM\": 1, \"HIGH\": 2}\n",
    "non_sin_mul = {\"NONE\": 0, \"SINGLE\": 1, \"MULTIPLE\": 2}\n",
    "loc_adj_net = {\"LOCAL\": 0, \"ADJACENT_NETWORK\": 1, \"NETWORK\": 2}\n",
    "\n",
    "ordinal_remapping = {\n",
    "    \"access_authentication\": non_sin_mul,\n",
    "    \"access_complexity\": low_med_hih,\n",
    "    \"access_vector\": loc_adj_net,\n",
    "    \"impact_availability\": non_par_com,\n",
    "    \"impact_confidentiality\": non_par_com,\n",
    "    \"impact_integrity\": non_par_com,\n",
    "}\n",
    "\n",
    "for ordinal_column in ordinal_remapping:\n",
    "    df[ordinal_column] = df[ordinal_column].apply(\n",
    "        lambda v: ordinal_remapping[ordinal_column].get(v, v)\n",
    "    )\n"
   ],
   "id": "1c8b34ce1e180904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Summary statistics for numeric columns (e.g., mean, min, max)\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())"
   ],
   "id": "a348d7c84411d0a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the column names\n",
    "print(df.columns.tolist())"
   ],
   "id": "8a59f8170a7c6081",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üßº 2. Handling Missing Data",
   "id": "1012c1ca37e6eca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for missing values in each column\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing Data Count:\\n\", missing_counts)\n",
    "\n",
    "# Remove rows with missing values\n",
    "cleaned_df = df.dropna()\n",
    "print(\"\\nCleaned DataFrame Head:\")\n",
    "print(cleaned_df.head())\n"
   ],
   "id": "197a3b652eb5af82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the 'summary' column as a NumPy array\n",
    "summary_array = df['summary'].to_numpy()\n",
    "\n",
    "# Show the first 10 values to understand the data\n",
    "print(summary_array[:10])"
   ],
   "id": "66abb14f2f34490e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  üß™ 3. Time-Series Data",
   "id": "ed28c4b360da8eb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert 'pub_date' column to datetime format\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'])  # Convert to datetime format\n",
    "df.set_index('pub_date', inplace=True) # Display the converted dates"
   ],
   "id": "5bcd8aebc0880e1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# General count of events over time\n",
    "df['pub_year'] = df.index.year\n",
    "year_x_count = df.groupby(\"pub_year\").size().reset_index(name=\"count\")\n",
    "fig = px.line(year_x_count, x=\"pub_year\", y=\"count\", log_y=True, title=\"CVE Code Count Over Time (Log Scale)\")\n",
    "fig.show()"
   ],
   "id": "b67b7f817d5c6ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract year, month, and day from the datetime index\n",
    "df['year'] = df.index.year  # Extract year\n",
    "df['month'] = df.index.month  # Extract month\n",
    "df['day'] = df.index.day  # Extract day\n",
    "print(df[['year', 'month', 'day']].head(10))  # Display the extracted columns\n",
    "\n",
    "# Count occurrences of each cwe_name per year\n",
    "cwe_counts = df.groupby(['year', 'cwe_name']).size().reset_index(name='count')\n",
    "\n",
    "# Get the most recent year\n",
    "most_recent_year = cwe_counts['year'].max()\n",
    "\n",
    "# Filter data for the most recent year\n",
    "cwe_counts_recent = cwe_counts[cwe_counts['year'] == most_recent_year]\n",
    "\n",
    "# Get the top 5 cwe_names by count in the most recent year\n",
    "top_5_cwe_names = cwe_counts_recent.nlargest(5, 'count')['cwe_name'].tolist()\n",
    "\n",
    "# Filter the original DataFrame for the top 5 cwe_names\n",
    "df_top5 = df[df['cwe_name'].isin(top_5_cwe_names)]\n",
    "\n",
    "# Count occurrences of each top cwe_name over all years\n",
    "top5_counts_over_time = df_top5.groupby(['year', 'cwe_name']).size().reset_index(name='count')\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=top5_counts_over_time, x='year', y='count', hue='cwe_name', marker='o')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top 5 CWE Names Over Time (Counts)\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a78cbc05114f8ea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate time differences from the earliest date\n",
    "print(\"Calculating Time Differences from the Start Date:\")\n",
    "df['Time_from_start'] = df.index - df.index.min()  # Calculate timedelta from the first date\n",
    "print(df[['Time_from_start']].head())  # Display the time differences"
   ],
   "id": "470b5e3d2e7284a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_cwe_code = 79\n",
    "# Create a time series of the counts of the target CWE\n",
    "time_series = df[df['cwe_code'] == target_cwe_code].groupby('year')['cwe_code'].count()\n",
    "\n",
    "# Convert the index to datetime (assuming the index represents years)\n",
    "# This is important for time series analysis functions\n",
    "time_series.index = pd.to_datetime(time_series.index, format='%Y')\n",
    "\n",
    "# Analyze Trend Component using Linear Regression\n",
    "# Convert datetime index to a numeric format for Linear Regression\n",
    "X = time_series.index.values.astype(np.int64).reshape(-1, 1)  # Use numeric representation of dates\n",
    "y = time_series.values  # Counts as target\n",
    "trend_model = LinearRegression()\n",
    "trend_model.fit(X, y)\n",
    "trend_line = trend_model.predict(X)\n",
    "\n",
    "# Time Series prediction using Exponential Smoothing\n",
    "# Consider if 'seasonal_periods=2' is appropriate for yearly data\n",
    "# If no clear 2-year cycle is expected, remove seasonal parameters or set seasonal=None\n",
    "model = ExponentialSmoothing(time_series, trend='add', seasonal='add', seasonal_periods=2)\n",
    "model_fit = model.fit()\n",
    "predict_steps = 5  # Number of years to forecast into the future\n",
    "predict = model_fit.forecast(steps=predict_steps)\n",
    "\n",
    "# Create a date range for the forecast period\n",
    "# Start from the end of the original time series index\n",
    "forecast_dates = pd.date_range(start=time_series.index[-1], periods=predict_steps + 1, freq='YS-JAN')[1:]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_series.index, time_series.values, label='Actual')\n",
    "plt.plot(time_series.index, trend_line, label='Trend (Linear Regression)', linestyle='--')\n",
    "plt.plot(forecast_dates, predict, label='Prediction (Exponential Smoothing)', linestyle='--', marker='o')  # Use forecast_dates\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count of CWE-79\")\n",
    "plt.title(\"CWE-79 Trend and Prediction\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
    "plt.show()"
   ],
   "id": "86aac1b8a1aca18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bce3bcc435c1fb54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìê 4. Probability Distribution & Descriptive Stats",
   "id": "78880aa4c8e4cc90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**The McCumber Cube is a model framework created by John McCumber in 1991 to help organizations establish and evaluate information security initiatives by considering all of the related factors that impact them.**\n",
    "\n",
    "**This security model has three dimensions:**\n",
    "\n",
    "**The foundational principles for protecting information systems.**\n",
    "\n",
    "**1.Availability, 2.Integrity, 3.Confidentiality ‚úÖ**\n",
    "\n",
    "**The protection of information in each of its possible states.**\n",
    "\n",
    "**The security measures used to protect data.**"
   ],
   "id": "e3dae08c619434d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Minimum and Maximum Values",
   "id": "6f3b798175c0d9c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Min and Max of DataFrame columns\n",
    "print(\"Minimum values in each column:\")\n",
    "print(df.min(numeric_only=True))\n",
    "\n",
    "print(\"\\nMaximum values in each column:\")\n",
    "print(df.max(numeric_only=True))\n"
   ],
   "id": "80985b7cc5134d9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "30512a55130f2172"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Median and Mode",
   "id": "ba3864de18f3085a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate median cwe_code\n",
    "median_cwe_code = df[\"cwe_code\"].median()\n",
    "print(f\"Median cwe_code: {median_cwe_code}\")\n",
    "\n",
    "# Calculate the mode of cwe_code\n",
    "mode_cwe_code = df[\"cwe_code\"].mode()\n",
    "print(f\"Most Common cwe_code: {mode_cwe_code[0]}, Count: {df['cwe_code'].value_counts()[mode_cwe_code[0]]}\")\n"
   ],
   "id": "6bfe9a4ffee78503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pie Chart: Top 3 codes by highest cvss scores\n",
    "\n",
    "# Select the top 3 codes with cvss scores\n",
    "df_latest = df[df['mod_date'] == df['mod_date'].max()]\n",
    "top3_codes = df_latest.nlargest(10, 'cvss')\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Set figure size\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(\n",
    "    top3_codes['cvss'],  # Values for the pie slices\n",
    "    labels=top3_codes['cwe_code'],  # Labels for each slice\n",
    "    autopct='%1.1f%%',  # Display percentage on slices\n",
    "    startangle=140,  # Rotate start angle for better layout\n",
    "    shadow=True  # Add shadow for visual effect\n",
    ")\n",
    "\n",
    "# Add a title\n",
    "plt.title('Top 3 Codes by Highest CVSS Score')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# CWE-732: Incorrect Permission Assignment for Critical Resource\n",
    "# Cross-Site Request Forgery (CSRF) - (352)\n",
    "# Weaknesses in the 2024 CWE Top 25 Most Dangerous Software Weaknesses"
   ],
   "id": "a494fbe1aa72ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e08b8fcd47295656"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.3 Quantiles and Interquartile Range (IQR)",
   "id": "b513a147b1144810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert 'pub_date' column to datetime format\n",
    "# df['pub_date'] = pd.to_datetime(df['pub_date']) , y=df[\"pub_date\"]\n",
    "# Calculate Q1 (25th percentile), Q3 (75th percentile), and IQR for cwe_code\n",
    "q1_cwe_code = df[\"cwe_code\"].quantile(0.25)\n",
    "q3_cwe_code = df[\"cwe_code\"].quantile(0.75)\n",
    "iqr_cwe_code = q3_cwe_code - q1_cwe_code\n",
    "\n",
    "print(f\"Q1 (25th percentile of cwe_code): {q1_cwe_code:.2f}\")\n",
    "print(f\"Q3 (75th percentile of cwe_code): {q3_cwe_code:.2f}\")\n",
    "print(f\"Interquartile Range (IQR) of cwe_code: {iqr_cwe_code:.2f}\")\n",
    "\n",
    "# Visualize IQR using a boxplot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=df[\"cwe_code\"])\n",
    "plt.title(\"Boxplot of cwe_code Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Weaknesses in the 2024 CWE Top 25 Most Dangerous Software Weaknesses\n",
    "# Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting') - (79)"
   ],
   "id": "4896f23f5b0bda25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìä 5.Visualising Relationships",
   "id": "c3fb7e946057c5a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üìä 5.1 Distribution Shapes",
   "id": "a1947c5b4265070a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Value counts of all entries in 'cwe_name' that contain 'Improper'\n",
    "improper_value_counts = df.loc[df['cwe_name'].str.contains('Improper', case=False, na=False), 'cwe_name'].value_counts()\n",
    "print(f\"\\nValue counts of 'cwe_name' entries containing 'Improper':\\n{improper_value_counts}\")\n",
    "\n",
    "# Create a new DataFrame from the value counts\n",
    "improper_counts_df = pd.DataFrame({'cwe_name': improper_value_counts.index, 'count': improper_value_counts.values})\n",
    "\n",
    "# Calculate mean, variance, and std of the 'val' column in improper_counts_df\n",
    "improper_mean_val = improper_counts_df['count'].mean()\n",
    "improper_var_val =  improper_counts_df['count'].var()     # sample variance\n",
    "improper_std_val =  improper_counts_df['count'].std()     # sample standard deviation\n",
    "\n",
    "print(f\"Mean (Expected Value) for Improper: {improper_mean_val:.4f}\")\n",
    "print(f\"Variance for Improper: {improper_var_val:.4f}\")\n",
    "print(f\"Standard Deviation for Improper: {improper_std_val:.4f}\")\n",
    "\n",
    "\n",
    "# Value counts of all entries in 'cwe_name' that contain 'Exposure'\n",
    "Exposure_value_counts = df.loc[df['cwe_name'].str.contains('Exposure', case=False, na=False), 'cwe_name'].value_counts()\n",
    "print(f\"\\nValue counts of 'cwe_name' entries containing 'Exposure':\\n{Exposure_value_counts}\")\n",
    "\n",
    "# Create a new DataFrame from the value counts\n",
    "Exposure_counts_df = pd.DataFrame({'cwe_name': Exposure_value_counts.index, 'count': Exposure_value_counts.values})\n",
    "\n",
    "# # Calculate mean, variance, and std of the 'val' column in Exposure_counts_df\n",
    "Exposure_mean_val = Exposure_counts_df['count'].mean()\n",
    "Exposure_var_val =  Exposure_counts_df['count'].var()     # sample variance\n",
    "Exposure_std_val =  Exposure_counts_df['count'].std()     # sample standard deviation\n",
    "\n",
    "print(f\"Mean (Expected Value) for Exposure: {Exposure_mean_val:.4f}\")\n",
    "print(f\"Variance for Exposure: {Exposure_var_val:.4f}\")\n",
    "print(f\"Standard Deviation for Exposure: {Exposure_std_val:.4f}\")\n",
    "\n",
    "# Create a DataFrame for the means and standard deviations\n",
    "stats = pd.DataFrame({\n",
    "    'Mean_Exposure': [Exposure_mean_val], \n",
    "    'Mean_Improper': [improper_mean_val],\n",
    "    'Std_Exposure': [Exposure_std_val],\n",
    "    'Std_Improper': [improper_std_val]\n",
    "})\n",
    "\n",
    "# Visualize distribution\n",
    "plt.hist(Exposure_counts_df['count'], bins=30, alpha=0.5, label='Exposure')\n",
    "plt.hist(improper_counts_df['count'], bins=30, alpha=0.5, label='Improper')\n",
    "plt.title(\"Histogram of 'Exposure' and 'Improper' Value Counts\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for the means\n",
    "means = pd.DataFrame({'Exposure': [Exposure_mean_val], 'Improper': [improper_mean_val]})\n",
    "\n",
    "# Create a DataFrame for the standard deviations\n",
    "stds = pd.DataFrame({'Exposure': [Exposure_std_val], 'Improper': [improper_std_val]})\n",
    "\n",
    "# Plot the bar chart with error bars\n",
    "means.plot.bar(yerr=stds, rot=0, capsize=4) # capsize adds caps to error bars\n",
    "plt.title(\"Mean with Standard Deviation Error Bars\")\n",
    "plt.xlabel('Exposure & Improper')\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "749c4a501a38356c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f9caf93600d0cff7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üìä 5.2 Poisson distribution",
   "id": "23c7e54d2b34b8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Estimate the mean of cvss\n",
    "lambda_value = df[\"cvss\"].mean()\n",
    "\n",
    "# Generate Poisson distribution\n",
    "poisson_data = np.random.poisson(lam=lambda_value, size=1000)\n",
    "\n",
    "# Plot distribution\n",
    "sns.histplot(poisson_data, kde=True, bins=30, color=\"blue\")\n",
    "plt.title(\"Poisson Distribution of cvss\")\n",
    "plt.xlabel(\"cvss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "c82e39fd69f51b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üì¶ 5.3 Detect, Report, and Visualize Outliers Using Z-Score",
   "id": "aff646c222f586fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detect, Report, and Visualize Outliers Using Z-Score\n",
    "\n",
    "def visualize_outliers(df, threshold=3):\n",
    "    df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "    # Calculate Z-scores\n",
    "    z_scores = df_numeric.apply(zscore, nan_policy='omit')\n",
    "\n",
    "    # Count how many values are considered outliers\n",
    "    outlier_counts = (z_scores.abs() > threshold).sum()\n",
    "    print(\"Number of outliers detected per column:\\n\", outlier_counts)\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    display(df_numeric.describe().T)\n",
    "\n",
    "    # Boxplot visualization\n",
    "    print(\"\\n Boxplots to Inspect Outliers:\")\n",
    "    df_numeric.plot(kind='box', subplots=True, layout=(1, len(df_numeric.columns)), figsize=(16, 4), patch_artist=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply outlier visualization\n",
    "visualize_outliers(df)\n"
   ],
   "id": "e594601fb8709972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  5.4 Analysing Correlation Between Variables",
   "id": "6bb7b80b3ed5dc75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  5.4.1 Covariance Analysis",
   "id": "c91e108daa23c625"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Covariance Analysis\n",
    "\n",
    "# Calculate a rolling mean to smooth out short-term fluctuations\n",
    "df['rolling_mean'] = df['cvss'].rolling(window=30).mean()\n",
    "\n",
    "# Remove rows with NaN values introduced by the rolling window\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Calculate the covariance matrix between the original values and their rolling mean\n",
    "cov_matrix = df_clean[['cvss', 'rolling_mean']].cov()\n",
    "\n",
    "# Display the covariance matrix\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Visualize the covariance matrix using a heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cov_matrix, annot=True, cmap=\"coolwarm\", center=0)\n",
    "\n",
    "# Add plot title\n",
    "plt.title(\"Covariance Matrix Heatmap\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "e4fd781c32e3708f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  5.4.2 Pearson: linear correlation & Spearman: rank-based correlation",
   "id": "754646a5d1e0a4a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate correlation matrices using different methods:\n",
    "\n",
    "pearson_corr = cov_matrix.corr(method='pearson') # - Pearson: linear correlation (assumes normality)\n",
    "spearman_corr = cov_matrix.corr(method='spearman') # - Spearman: rank-based correlation (monotonic relationships)\n",
    "kendall_corr = cov_matrix.corr(method='kendall') # - Kendall: rank correlation (more robust with small samples or ties)\n",
    "\n",
    "# Display the correlation matrices\n",
    "print(\"\\nPearson Correlation:\")\n",
    "print(pearson_corr)\n",
    "\n",
    "print(\"\\nSpearman Correlation:\")\n",
    "print(spearman_corr)\n",
    "\n",
    "print(\"\\nKendall Correlation:\")\n",
    "print(kendall_corr)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(pearson_corr, annot=True, cmap=\"coolwarm\", center=0)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Pearson Correlation Heatmap\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ],
   "id": "70e56d454e0f86b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  5.4.3 Linear regression model",
   "id": "fadeda3aec31b9c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Choose a specific cwe_code to predict\n",
    "target_cwe_code = 79  # Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')\n",
    "\n",
    "# Create a binary target variable (1 if the cwe_code is the target, 0 otherwise)\n",
    "df['target'] = (df['cwe_code'] == target_cwe_code).astype(int)\n",
    "\n",
    "# Drop rows with missing values in 'year'\n",
    "df_clean = df.dropna(subset=['year']).copy() # Added .copy() to avoid SettingWithCopyWarning\n",
    "df_clean['year'] = pd.to_datetime(df_clean['year'])\n",
    "# Convert datetime to numeric for regression (using seconds since epoch)\n",
    "\n",
    "df_clean['year_numeric'] = (df_clean['year'] - df_clean['year'].min()).dt.total_seconds()\n",
    "\n",
    "# Define features and target\n",
    "X = df_clean[['year_numeric']]\n",
    "y = df_clean['target']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict values for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"\\nModel Evaluation:\")\n",
    " # The constant term (Œ≤‚ÇÄ)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "# The slope (Œ≤‚ÇÅ) for rolling_mean\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "# Proportion of variance explained\n",
    "print(\"R-squared:\", r2_score(y_test, y_pred))\n",
    "# Average squared error\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 5))  # Set the figure size\n",
    "\n",
    "# Scatter plot of actual test data\n",
    "plt.scatter(y_test, y_pred, label='Actual vs. Predicted', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],'k--', label='Ideal Prediction (y=x)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Actual target (Is CWE 79?)\")\n",
    "plt.ylabel(\"Predicted target probability\")\n",
    "plt.title(\"Linear Regression: Actual vs. Predicted Target\")\n",
    "\n",
    "# Add legend and show the plot\n",
    "plt.legend()\n",
    "plt.grid(True) # Add grid for better readability\n",
    "plt.show()"
   ],
   "id": "d49136945d64b2d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Recommendations:\n",
    "# \n",
    "# Prioritize Remediation: Focus on addressing the highest-risk categories, especially injection flaws and broken access control.\n",
    "# Secure Coding Practices: Implement secure coding practices to prevent these vulnerabilities from being introduced in the first place.\n",
    "# Regular Security Testing: Conduct regular security testing (e.g., penetration testing, static analysis) to identify and address vulnerabilities early.\n",
    "# Security Training: Provide security training to developers to raise awareness and improve secure coding skills."
   ],
   "id": "b9f03cd1034bf5b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  5.4.3 Linear regression model",
   "id": "f4224e6890999cec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc # Import classification metrics\n",
    "\n",
    "# Choose a specific cwe_code to predict\n",
    "target_cwe_code = 79  # Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')\n",
    "\n",
    "# Create a binary target variable (1 if the cwe_code is the target, 0 otherwise)\n",
    "# outcome we want to predict\n",
    "df['target'] = (df['cwe_code'] == target_cwe_code).astype(int)\n",
    "\n",
    "# Drop rows with missing values in 'year'\n",
    "# df_clean = df.dropna(subset=['year']).copy() # Added .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Ensure the 'year' column is datetime type\n",
    "# df_clean['year'] = pd.to_datetime(df_clean['year'])\n",
    "\n",
    "# Convert datetime to numeric for regression (using seconds since epoch)\n",
    "df_clean['year_numeric'] = (df_clean['year'] - df_clean['year'].min()).dt.total_seconds()\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = df_clean[['year_numeric']]\n",
    "y = df_clean['target']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Proportion of target=1 in training set: {y_train.mean():.4f}\")\n",
    "print(f\"Proportion of target=1 in testing set: {y_test.mean():.4f}\")\n",
    "\n",
    "\n",
    "# Initialize and fit a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Model Training Complete ---\")\n",
    "# Print model coefficients and intercept for interpretation\n",
    "print(f\"Model coefficient (for year_numeric): {model.coef_[0][0]:.4f}\")\n",
    "print(f\"Model intercept: {model.intercept_[0]:.4f}\")\n",
    "\n",
    "\n",
    "# Predict class labels for the test set\n",
    "y_pred_class = model.predict(X_test)\n",
    "\n",
    "# Predict probabilities of the positive class (CWE 79) for the test set\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# Evaluate model performance using classification metrics\n",
    "print(\"\\n--- Model Evaluation (Logistic Regression) ---\")\n",
    "\n",
    "# Accuracy: Overall proportion of correct predictions\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix: Breakdown of correct and incorrect predictions per class\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report: Provides Precision, Recall, F1-Score for each class\n",
    "class_report = classification_report(y_test, y_pred_class, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot actual points (jittered on the y-axis for better visualization if many points overlap)\n",
    "\n",
    "plt.scatter(X_test['year_numeric'], y_test + np.random.uniform(-0.05, 0.05, size=y_test.shape),\n",
    "            color='grey', label='Actual (0=No, 1=Yes)', alpha=0.6)\n",
    "\n",
    "# Sort the test data by year_numeric to plot a smooth probability curve\n",
    "X_test_sorted = X_test.sort_values(by='year_numeric')\n",
    "y_pred_proba_sorted = model.predict_proba(X_test_sorted)[:, 1]\n",
    "\n",
    "# Plot the predicted probability curve from the Logistic Regression model\n",
    "plt.plot(X_test_sorted['year_numeric'], y_pred_proba_sorted, color='red', label='Predicted Probability (Logistic Regression)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Year (Numeric - Seconds since earliest year)\")\n",
    "plt.ylabel(\"Predicted Probability of being CWE 79\")\n",
    "plt.title(\"Logistic Regression: Predicted Probability of CWE 79 over Time\")\n",
    "\n",
    "# Add legend and show the plot\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Diagonal line representing random chance\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "bffb23a56f524131",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
